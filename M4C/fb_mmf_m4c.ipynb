{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/facebookresearch/mmf.git\n",
    "#! cd mmf\n",
    "#! pip install --editable .\n",
    "# ! pip install urllib3==1.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/textvqa/defaults.yaml\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.textvqa.alone\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/textvqa/defaults.yaml', 'datasets=textvqa', 'model=m4c', 'run_type=val', 'checkpoint.resume_zoo=m4c.textvqa.alone', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf_cli.run: \u001b[0mUsing seed 9283845\n",
      "\u001b[32m2020-11-02T21:41:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-02T21:41:12 | transformers.tokenization_utils: \u001b[0mloading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-02T21:41:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-02T21:41:17 | transformers.modeling_utils: \u001b[0mloading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-02T21:41:20 | transformers.modeling_utils: \u001b[0mWeights from pretrained model not used in TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-02T21:41:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-02T21:41:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:27 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:27 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:27 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:27 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-02T21:41:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/m4c/m4c.textvqa.alone.tar.gz to /home/ubuntu/.cache/torch/mmf/data/models/m4c.textvqa.alone/m4c.textvqa.alone.tar.gz ]\n",
      "Downloading m4c.textvqa.alone.tar.gz: 100%|██| 336M/336M [00:14<00:00, 23.2MB/s]\n",
      "[ Starting checksum for m4c.textvqa.alone.tar.gz]\n",
      "[ Checksum successful for m4c.textvqa.alone.tar.gz]\n",
      "Unpacking m4c.textvqa.alone.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-02T21:41:48 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.common.test_reporter: \u001b[0mPredicting for textvqa\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[32m2020-11-02T21:41:48 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\u001b[32m2020-11-02T21:41:48 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/40 [00:10<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 779, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 2009) is killed by signal: Killed. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/bin/mmf_predict\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_predict')())\n",
      "  File \"/home/ubuntu/mmf/mmf_cli/predict.py\", line 15, in predict\n",
      "    run(predict=True)\n",
      "  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 122, in run\n",
      "    main(configuration, predict=predict)\n",
      "  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 54, in main\n",
      "    trainer.inference()\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/mmf_trainer.py\", line 138, in inference\n",
      "    self.prediction_loop(dataset)\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/core/evaluation_loop.py\", line 62, in prediction_loop\n",
      "    for batch in tqdm.tqdm(dataloader):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tqdm/std.py\", line 1171, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 974, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 792, in _try_get_data\n",
      "    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str))\n",
      "RuntimeError: DataLoader worker (pid(s) 2009) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "#! mmf_predict config=projects/m4c/configs/textvqa/defaults.yaml \\\n",
    "    datasets=textvqa \\\n",
    "    model=m4c \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_zoo=m4c.textvqa.alone\n",
    "#dataloader fails -> memory issues; need training.num_workers=1 I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/textvqa/joint_with_stvqa.yaml\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.textvqa.with_stvqa\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 1\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf: \u001b[0mLogging to: ./save/m4c/train.log\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['dataset=textvqa', 'model=m4c', 'config=projects/m4c/configs/textvqa/joint_with_stvqa.yaml', 'env.save_dir=./save/m4c', 'run_type=val', 'checkpoint.resume_zoo=m4c.textvqa.with_stvqa', 'training.num_workers=1'])\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf_cli.run: \u001b[0mUsing seed 20769285\n",
      "\u001b[32m2020-11-02T23:53:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100%|█████████| 7.42G/7.42G [05:07<00:00, 24.1MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/annotations/annotations.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/annotations/annotations.tar.gz ]\n",
      "Downloading annotations.tar.gz: 100%|██████| 46.5M/46.5M [00:02<00:00, 15.6MB/s]\n",
      "[ Starting checksum for annotations.tar.gz]\n",
      "[ Checksum successful for annotations.tar.gz]\n",
      "Unpacking annotations.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/extras.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100%|██████████████| 230k/230k [00:00<00:00, 304kB/s]\n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/ocr_en/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/ocr_en/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100%|███████████| 617M/617M [01:54<00:00, 5.41MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "\u001b[32m2020-11-03T00:03:08 | transformers.tokenization_utils: \u001b[0mloading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-03T00:03:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-03T00:03:20 | transformers.modeling_utils: \u001b[0mloading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-03T00:03:22 | transformers.modeling_utils: \u001b[0mWeights from pretrained model not used in TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-03T00:03:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-03T00:03:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:29 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:29 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:29 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:29 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-03T00:03:29 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/m4c/m4c.textvqa.with_stvqa.tar.gz to /home/ubuntu/.cache/torch/mmf/data/models/m4c.textvqa.with_stvqa/m4c.textvqa.with_stvqa.tar.gz ]\n",
      "Downloading m4c.textvqa.with_stvqa.tar.gz: 100%|█| 336M/336M [00:14<00:00, 22.8M\n",
      "[ Starting checksum for m4c.textvqa.with_stvqa.tar.gz]\n",
      "[ Checksum successful for m4c.textvqa.with_stvqa.tar.gz]\n",
      "Unpacking m4c.textvqa.with_stvqa.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:03:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.trainers.mmf_trainer: \u001b[0mM4C(\n",
      "  (text_bert): TextBert(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_bert_out_linear): Identity()\n",
      "  (obj_faster_rcnn_fc7): FinetuneFasterRcnnFpnFc7(\n",
      "    (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      "  (linear_obj_feat_to_mmt_in): Linear(in_features=2048, out_features=768, bias=True)\n",
      "  (linear_obj_bbox_to_mmt_in): Linear(in_features=4, out_features=768, bias=True)\n",
      "  (obj_feat_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (obj_bbox_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (obj_drop): Dropout(p=0.1, inplace=False)\n",
      "  (ocr_faster_rcnn_fc7): FinetuneFasterRcnnFpnFc7(\n",
      "    (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      "  (linear_ocr_feat_to_mmt_in): Linear(in_features=3002, out_features=768, bias=True)\n",
      "  (linear_ocr_bbox_to_mmt_in): Linear(in_features=4, out_features=768, bias=True)\n",
      "  (ocr_feat_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (ocr_bbox_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (ocr_drop): Dropout(p=0.1, inplace=False)\n",
      "  (mmt): MMT(\n",
      "    (prev_pred_embeddings): PrevPredEmbeddings(\n",
      "      (position_embeddings): Embedding(100, 768)\n",
      "      (token_type_embeddings): Embedding(5, 768)\n",
      "      (ans_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (ocr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (emb_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (emb_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ocr_ptr_net): OcrPtrNet(\n",
      "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (classifier): ClassifierLayer(\n",
      "    (module): Linear(in_features=768, out_features=5000, bias=True)\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): M4CDecodingBCEWithMaskLoss()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.utils.general: \u001b[0mTotal Parameters: 90850184. Trained Parameters: 90850184\n",
      "\u001b[32m2020-11-03T00:03:50 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[32m2020-11-03T00:03:50 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-03T00:04:31 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "100%|███████████████████████████████████████████| 40/40 [10:25<00:00, 15.64s/it]\n",
      "\u001b[32m2020-11-03T00:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mval/textvqa/m4c_decoding_bce_with_mask: 6.1611, val/total_loss: 6.1611, val/textvqa/textvqa_accuracy: 0.3882\n",
      "\u001b[32m2020-11-03T00:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 10m 48s 822ms\n"
     ]
    }
   ],
   "source": [
    "! mmf_run dataset=textvqa model=m4c \\\n",
    "    config=projects/m4c/configs/textvqa/joint_with_stvqa.yaml \\\n",
    "    env.save_dir=./save/m4c \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_zoo=m4c.textvqa.with_stvqa \\\n",
    "    training.num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/textvqa/defaults.yaml\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.textvqa.alone\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c_textvqa_predict_val\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 1\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf: \u001b[0mLogging to: ./save/m4c_textvqa_predict_val/train.log\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/textvqa/defaults.yaml', 'datasets=textvqa', 'model=m4c', 'run_type=val', 'checkpoint.resume_zoo=m4c.textvqa.alone', 'env.save_dir=./save/m4c_textvqa_predict_val', 'training.num_workers=1', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf_cli.run: \u001b[0mUsing seed 3966183\n",
      "\u001b[32m2020-11-03T00:21:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-03T00:21:07 | transformers.tokenization_utils: \u001b[0mloading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-03T00:21:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-03T00:21:12 | transformers.modeling_utils: \u001b[0mloading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-03T00:21:15 | transformers.modeling_utils: \u001b[0mWeights from pretrained model not used in TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-03T00:21:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-03T00:21:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-03T00:21:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:21:25 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-03T00:21:25 | mmf.common.test_reporter: \u001b[0mPredicting for textvqa\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[32m2020-11-03T00:21:25 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-03T00:22:04 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "100%|███████████████████████████████████████████| 40/40 [08:48<00:00, 13.22s/it]\n",
      "\u001b[32m2020-11-03T00:30:14 | mmf.common.test_reporter: \u001b[0mWrote predictions for textvqa to /home/ubuntu/save/m4c_textvqa_predict_val/textvqa_m4c_3966183/reports/textvqa_run_val_2020-11-03T00:30:14.json\n",
      "\u001b[32m2020-11-03T00:30:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "! mmf_predict config=projects/m4c/configs/textvqa/defaults.yaml \\\n",
    "    datasets=textvqa \\\n",
    "    model=m4c \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_zoo=m4c.textvqa.alone \\\n",
    "    env.save_dir=./save/m4c_textvqa_predict_val \\\n",
    "    training.num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/stvqa/defaults.yaml\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to stvqa\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.stvqa.defaults\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c_stvqa_predict_val\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 1\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf: \u001b[0mLogging to: ./save/m4c_stvqa_predict_val/train.log\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/stvqa/defaults.yaml', 'datasets=stvqa', 'model=m4c', 'run_type=val', 'checkpoint.resume_zoo=m4c.stvqa.defaults', 'env.save_dir=./save/m4c_stvqa_predict_val', 'training.num_workers=1', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf_cli.run: \u001b[0mUsing seed 47905420\n",
      "\u001b[32m2020-11-03T00:35:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-03T00:35:50 | transformers.tokenization_utils: \u001b[0mloading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-03T00:35:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-03T00:35:55 | transformers.modeling_utils: \u001b[0mloading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-03T00:35:58 | transformers.modeling_utils: \u001b[0mWeights from pretrained model not used in TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-03T00:36:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-03T00:36:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:03 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:03 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:03 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:03 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-03T00:36:03 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/m4c/m4c.stvqa.tar.gz to /home/ubuntu/.cache/torch/mmf/data/models/m4c.stvqa.defaults/m4c.stvqa.tar.gz ]\n",
      "Downloading m4c.stvqa.tar.gz: 100%|██████████| 336M/336M [00:14<00:00, 22.6MB/s]\n",
      "[ Starting checksum for m4c.stvqa.tar.gz]\n",
      "[ Checksum successful for m4c.stvqa.tar.gz]\n",
      "Unpacking m4c.stvqa.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:235: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-03T00:36:24 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:278: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-03T00:36:24 | mmf.common.test_reporter: \u001b[0mPredicting for stvqa\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[32m2020-11-03T00:36:25 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-03T00:37:03 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "100%|███████████████████████████████████████████| 21/21 [03:49<00:00, 10.93s/it]\n",
      "\u001b[32m2020-11-03T00:40:14 | mmf.common.test_reporter: \u001b[0mWrote predictions for stvqa to /home/ubuntu/save/m4c_stvqa_predict_val/stvqa_m4c_47905420/reports/stvqa_run_val_2020-11-03T00:40:14.json\n",
      "\u001b[32m2020-11-03T00:40:14 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "! mmf_predict config=projects/m4c/configs/stvqa/defaults.yaml \\\n",
    "    datasets=stvqa \\\n",
    "    model=m4c \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_zoo=m4c.stvqa.defaults \\\n",
    "    env.save_dir=./save/m4c_stvqa_predict_val \\\n",
    "    training.num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mmf.sh/docs/projects/m4c/#pretrained-m4c-models%20(important%20for%20arguments!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
