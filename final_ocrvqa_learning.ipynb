{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"final_assignment.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"88lYF9an4k-C"},"source":["LINKS:\n","https://mmf.sh/docs/tutorials/checkpointing/\n","https://github.com/facebookresearch/mmf/issues/707"]},{"cell_type":"code","metadata":{"id":"3Pe3EYyU29U9","outputId":"4429ab68-f13a-4896-847f-4498e2b0e62c"},"source":["#DO ALL THESE IN THE COMMAND LINE\n","#! git clone https://github.com/facebookresearch/mmf.git\n","#! cd mmf\n","#! pip install --editable .\n","! pip install urllib3==1.25.4"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: urllib3==1.25.4 in ./anaconda3/lib/python3.7/site-packages (1.25.4)\n","\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.1 is available.\n","You should consider upgrading via the '/home/ubuntu/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h1pRqgZB29VH"},"source":["below is simply to download ocrvqa dataset \n","we need ocrvqa file thats why we have to download ocrvqa dataset -> actually LOL do not need. to do. this!"]},{"cell_type":"code","metadata":{"id":"bTkxSl6929VI","outputId":"f83ddf99-ff48-44a3-f684-8e675f6a24e2"},"source":["! mmf_run config=projects/m4c/configs/ocrvqa/defaults.yaml \\\n","    datasets=ocrvqa \\\n","    model=m4c \\\n","    run_type=train_val \\\n","    checkpoint.resume_pretrained=True \\\n","    checkpoint.resume_zoo=m4c.ocrvqa.defaults \\\n","    env.save_dir=./save/m4c_textvqa_predict_val"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/ocrvqa/defaults.yaml\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option datasets to ocrvqa\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.ocrvqa.defaults\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c_textvqa_predict_val\n","\u001b[32m2020-12-08T22:17:34 | mmf: \u001b[0mLogging to: ./save/m4c_textvqa_predict_val/train.log\n","\u001b[32m2020-12-08T22:17:34 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/ocrvqa/defaults.yaml', 'datasets=ocrvqa', 'model=m4c', 'run_type=train_val', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=m4c.ocrvqa.defaults', 'env.save_dir=./save/m4c_textvqa_predict_val'])\n","\u001b[32m2020-12-08T22:17:34 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n","\u001b[32m2020-12-08T22:17:34 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n","\u001b[32m2020-12-08T22:17:34 | mmf_cli.run: \u001b[0mUsing seed 34353412\n","\u001b[32m2020-12-08T22:17:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/ocrvqa/defaults/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/ocrvqa/defaults/features/features.tar.gz ]\n","Downloading features.tar.gz: 100%|█████████| 69.7G/69.7G [47:29<00:00, 24.4MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/ocrvqa/defaults/annotations/annotations.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/ocrvqa/defaults/annotations/annotations.tar.gz ]\n","Downloading annotations.tar.gz: 100%|████████| 691M/691M [00:31<00:00, 22.3MB/s]\n","[ Starting checksum for annotations.tar.gz]\n","[ Checksum successful for annotations.tar.gz]\n","Unpacking annotations.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/ocrvqa/defaults/extras.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/ocrvqa/defaults/extras.tar.gz ]\n","Downloading extras.tar.gz: 100%|██████████████| 211k/211k [00:00<00:00, 301kB/s]\n","[ Starting checksum for extras.tar.gz]\n","[ Checksum successful for extras.tar.gz]\n","Unpacking extras.tar.gz\n","[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/ocrvqa/ocr_en/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/ocrvqa/ocr_en/features/features.tar.gz ]\n","Downloading features.tar.gz: 100%|█████████| 12.8G/12.8G [08:42<00:00, 24.5MB/s]\n","[ Starting checksum for features.tar.gz]\n","[ Checksum successful for features.tar.gz]\n","Unpacking features.tar.gz\n","^C\n","Traceback (most recent call last):\n","  File \"/home/ubuntu/anaconda3/bin/mmf_run\", line 33, in <module>\n","    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n","  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 122, in run\n","    main(configuration, predict=predict)\n","  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 52, in main\n","    trainer.load()\n","  File \"/home/ubuntu/mmf/mmf/trainers/mmf_trainer.py\", line 42, in load\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PTDP681z3fDH"},"source":["https://github.com/facebookresearch/mmf/issues/707 -> lol author of textvqa answered my question\n","\n","so in mmf/configs/zoo/defaults.yaml\n","\n","add in \n","\n","checkpoint:\n","  pretrained_state_mapping:\n","    model.text_bert: model.text_bert\n","    model.obj: model.obj\n","    model.ocr: model.ocr\n","    model.mmt: model.mmt\n","\n","aka add in all layers but the classifier"]},{"cell_type":"markdown","metadata":{"id":"rEuR1kG84KGc"},"source":["this is to start training: (do it on a p2.xlarge instance)\n","probably need at least 500gb...potentially 1000gb to do this (ebs volumes)"]},{"cell_type":"code","metadata":{"id":"0K-PW3mM29VJ","outputId":"9d21454a-8a59-4f85-d6c7-c0d5f262b0f5"},"source":["! mmf_run config=projects/m4c/configs/textvqa/defaults.yaml \\\n","    datasets=textvqa \\\n","    model=m4c \\\n","    run_type=train_val \\\n","    checkpoint.resume_pretrained=True \\\n","    checkpoint.resume_zoo=m4c.ocrvqa.defaults \\\n","    env.save_dir=./save/m4c_final_yay \\\n","    training.num_workers=6 \\\n","    training.batch_size=80 \\\n","    training.max_updates=25000 \\\n","    training.log_interval=10 \\\n","    training.checkpoint_interval=100 \\\n","    training.evaluation_interval=2000"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.text_bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.obj) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.ocr) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.mmt) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/textvqa/defaults.yaml\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.ocrvqa.defaults\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c_final_yay\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 6\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 80\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 25000\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 100\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 2000\n","\u001b[32m2020-12-09T07:22:23 | mmf: \u001b[0mLogging to: ./save/m4c_final_yay/train.log\n","\u001b[32m2020-12-09T07:22:23 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/textvqa/defaults.yaml', 'datasets=textvqa', 'model=m4c', 'run_type=train_val', 'checkpoint.resume_pretrained=True', 'checkpoint.resume_zoo=m4c.ocrvqa.defaults', 'env.save_dir=./save/m4c_final_yay', 'training.num_workers=6', 'training.batch_size=80', 'training.max_updates=25000', 'training.log_interval=10', 'training.checkpoint_interval=100', 'training.evaluation_interval=2000'])\n","\u001b[32m2020-12-09T07:22:23 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n","\u001b[32m2020-12-09T07:22:23 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla K80\n","\u001b[32m2020-12-09T07:22:23 | mmf_cli.run: \u001b[0mUsing seed 23124074\n","\u001b[32m2020-12-09T07:22:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n","\u001b[32m2020-12-09T07:22:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n","- This IS expected if you are initializing TextBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TextBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[32m2020-12-09T07:22:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n","\u001b[32m2020-12-09T07:22:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:44 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n","  \"No type for scheduler specified even though lr_scheduler is True, \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:44 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n","  \"No type for scheduler specified even though lr_scheduler is True, \"\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:44 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n","  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:44 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n","  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n","\n","\u001b[32m2020-12-09T07:22:44 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:22:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n","\u001b[32m2020-12-09T07:22:49 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n","\u001b[32m2020-12-09T07:22:49 | mmf.trainers.mmf_trainer: \u001b[0mM4C(\n","  (text_bert): TextBert(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (text_bert_out_linear): Identity()\n","  (obj_faster_rcnn_fc7): FinetuneFasterRcnnFpnFc7(\n","    (lc): Linear(in_features=2048, out_features=2048, bias=True)\n","  )\n","  (linear_obj_feat_to_mmt_in): Linear(in_features=2048, out_features=768, bias=True)\n","  (linear_obj_bbox_to_mmt_in): Linear(in_features=4, out_features=768, bias=True)\n","  (obj_feat_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (obj_bbox_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (obj_drop): Dropout(p=0.1, inplace=False)\n","  (ocr_faster_rcnn_fc7): FinetuneFasterRcnnFpnFc7(\n","    (lc): Linear(in_features=2048, out_features=2048, bias=True)\n","  )\n","  (linear_ocr_feat_to_mmt_in): Linear(in_features=3002, out_features=768, bias=True)\n","  (linear_ocr_bbox_to_mmt_in): Linear(in_features=4, out_features=768, bias=True)\n","  (ocr_feat_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (ocr_bbox_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (ocr_drop): Dropout(p=0.1, inplace=False)\n","  (mmt): MMT(\n","    (prev_pred_embeddings): PrevPredEmbeddings(\n","      (position_embeddings): Embedding(100, 768)\n","      (token_type_embeddings): Embedding(5, 768)\n","      (ans_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (ocr_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (emb_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (emb_dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (ocr_ptr_net): OcrPtrNet(\n","    (query): Linear(in_features=768, out_features=768, bias=True)\n","    (key): Linear(in_features=768, out_features=768, bias=True)\n","  )\n","  (classifier): ClassifierLayer(\n","    (module): Linear(in_features=768, out_features=5000, bias=True)\n","  )\n","  (losses): Losses(\n","    (losses): ModuleList(\n","      (0): MMFLoss(\n","        (loss_criterion): M4CDecodingBCEWithMaskLoss()\n","      )\n","    )\n","  )\n",")\n","\u001b[32m2020-12-09T07:22:49 | mmf.utils.general: \u001b[0mTotal Parameters: 90850184. Trained Parameters: 90850184\n","\u001b[32m2020-12-09T07:22:49 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[32m2020-12-09T07:22:49 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\u001b[32m2020-12-09T07:22:49 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\n","\n","\u001b[32m2020-12-09T07:22:49 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\n","\u001b[32m2020-12-09T07:22:49 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\n","\u001b[32m2020-12-09T07:22:50 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\u001b[32m2020-12-09T07:22:50 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n","\n","\n","\u001b[32m2020-12-09T07:24:06 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:24:06 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:24:06 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:24:06 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:24:06 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:24:07 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n","\u001b[32m2020-12-09T07:26:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10/25000, train/textvqa/m4c_decoding_bce_with_mask: 3139.7832, train/textvqa/m4c_decoding_bce_with_mask/avg: 3139.7832, train/total_loss: 3139.7832, train/total_loss/avg: 3139.7832, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 10, iterations: 10, max_updates: 25000, lr: 0.00002, ups: 0.05, time: 03m 27s 812ms, time_since_start: 03m 32s 762ms, eta: 144h 15m 24s 067ms\n","\u001b[32m2020-12-09T07:28:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20/25000, train/textvqa/m4c_decoding_bce_with_mask: 2739.0142, train/textvqa/m4c_decoding_bce_with_mask/avg: 2939.3987, train/total_loss: 2739.0142, train/total_loss/avg: 2939.3987, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 20, iterations: 20, max_updates: 25000, lr: 0.00002, ups: 0.09, time: 01m 52s 924ms, time_since_start: 05m 25s 687ms, eta: 78h 21m 25s 818ms\n","\u001b[32m2020-12-09T07:29:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 30/25000, train/textvqa/m4c_decoding_bce_with_mask: 2739.0142, train/textvqa/m4c_decoding_bce_with_mask/avg: 2762.2500, train/total_loss: 2739.0142, train/total_loss/avg: 2762.2500, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 30, iterations: 30, max_updates: 25000, lr: 0.00002, ups: 0.17, time: 01m 765ms, time_since_start: 06m 26s 452ms, eta: 42h 08m 51s 713ms\n","\u001b[32m2020-12-09T07:31:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 40/25000, train/textvqa/m4c_decoding_bce_with_mask: 2407.9526, train/textvqa/m4c_decoding_bce_with_mask/avg: 2594.6749, train/total_loss: 2407.9526, train/total_loss/avg: 2594.6749, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 40, iterations: 40, max_updates: 25000, lr: 0.00002, ups: 0.09, time: 01m 49s 471ms, time_since_start: 08m 15s 924ms, eta: 75h 54m 979ms\n","\u001b[32m2020-12-09T07:32:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/25000, train/textvqa/m4c_decoding_bce_with_mask: 2407.9526, train/textvqa/m4c_decoding_bce_with_mask/avg: 2434.7804, train/total_loss: 2407.9526, train/total_loss/avg: 2434.7804, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 50, iterations: 50, max_updates: 25000, lr: 0.00002, ups: 0.09, time: 01m 52s 870ms, time_since_start: 10m 08s 794ms, eta: 78h 13m 30s 874ms\n","\u001b[32m2020-12-09T07:33:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 60/25000, train/textvqa/m4c_decoding_bce_with_mask: 2091.9497, train/textvqa/m4c_decoding_bce_with_mask/avg: 2282.5956, train/total_loss: 2091.9497, train/total_loss/avg: 2282.5956, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 60, iterations: 60, max_updates: 25000, lr: 0.00002, ups: 0.15, time: 01m 05s 354ms, time_since_start: 11m 14s 149ms, eta: 45h 16m 35s 345ms\n","\u001b[32m2020-12-09T07:35:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 70/25000, train/textvqa/m4c_decoding_bce_with_mask: 2091.9497, train/textvqa/m4c_decoding_bce_with_mask/avg: 2138.6813, train/total_loss: 2091.9497, train/total_loss/avg: 2138.6813, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 70, iterations: 70, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 50s 593ms, time_since_start: 13m 04s 743ms, eta: 76h 35m 10s 322ms\n","\u001b[32m2020-12-09T07:37:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 80/25000, train/textvqa/m4c_decoding_bce_with_mask: 1795.2024, train/textvqa/m4c_decoding_bce_with_mask/avg: 2003.6741, train/total_loss: 1795.2024, train/total_loss/avg: 2003.6741, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 80, iterations: 80, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 51s 695ms, time_since_start: 14m 56s 438ms, eta: 77h 19m 05s 231ms\n","\u001b[32m2020-12-09T07:38:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 90/25000, train/textvqa/m4c_decoding_bce_with_mask: 1795.2024, train/textvqa/m4c_decoding_bce_with_mask/avg: 1877.7276, train/total_loss: 1795.2024, train/total_loss/avg: 1877.7276, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 90, iterations: 90, max_updates: 25000, lr: 0.00003, ups: 0.15, time: 01m 05s 425ms, time_since_start: 16m 01s 864ms, eta: 45h 16m 15s 521ms\n","\u001b[32m2020-12-09T07:40:38 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:40:38 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:40:38 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","\n","\u001b[32m2020-12-09T07:40:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/25000, train/textvqa/m4c_decoding_bce_with_mask: 1521.6715, train/textvqa/m4c_decoding_bce_with_mask/avg: 1760.9790, train/total_loss: 1521.6715, train/total_loss/avg: 1760.9790, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 25000, lr: 0.00003, ups: 0.08, time: 02m 804ms, time_since_start: 18m 02s 668ms, eta: 83h 33m 22s 485ms\n","\u001b[32m2020-12-09T07:42:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 110/25000, train/textvqa/m4c_decoding_bce_with_mask: 1521.6715, train/textvqa/m4c_decoding_bce_with_mask/avg: 1653.1012, train/total_loss: 1521.6715, train/total_loss/avg: 1653.1012, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 110, iterations: 110, max_updates: 25000, lr: 0.00003, ups: 0.08, time: 02m 04s 089ms, time_since_start: 20m 06s 758ms, eta: 85h 47m 39s 943ms\n","\u001b[32m2020-12-09T07:43:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 120/25000, train/textvqa/m4c_decoding_bce_with_mask: 1275.1953, train/textvqa/m4c_decoding_bce_with_mask/avg: 1553.7771, train/total_loss: 1275.1953, train/total_loss/avg: 1553.7771, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 120, iterations: 120, max_updates: 25000, lr: 0.00003, ups: 0.15, time: 01m 06s 775ms, time_since_start: 21m 13s 533ms, eta: 46h 08m 57s 002ms\n","\u001b[32m2020-12-09T07:45:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 130/25000, train/textvqa/m4c_decoding_bce_with_mask: 1275.1953, train/textvqa/m4c_decoding_bce_with_mask/avg: 1462.5791, train/total_loss: 1275.1953, train/total_loss/avg: 1462.5791, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 130, iterations: 130, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 53s 702ms, time_since_start: 23m 07s 236ms, eta: 78h 32m 57s 155ms\n","\u001b[32m2020-12-09T07:47:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 140/25000, train/textvqa/m4c_decoding_bce_with_mask: 1058.6235, train/textvqa/m4c_decoding_bce_with_mask/avg: 1378.9484, train/total_loss: 1058.6235, train/total_loss/avg: 1378.9484, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 140, iterations: 140, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 52s 199ms, time_since_start: 24m 59s 435ms, eta: 77h 28m 48s 057ms\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[32m2020-12-09T07:48:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/25000, train/textvqa/m4c_decoding_bce_with_mask: 1058.6235, train/textvqa/m4c_decoding_bce_with_mask/avg: 1302.4149, train/total_loss: 1058.6235, train/total_loss/avg: 1302.4149, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 150, iterations: 150, max_updates: 25000, lr: 0.00003, ups: 0.15, time: 01m 06s 297ms, time_since_start: 26m 05s 732ms, eta: 45h 45m 48s 756ms\n","\u001b[32m2020-12-09T07:50:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 160/25000, train/textvqa/m4c_decoding_bce_with_mask: 870.1559, train/textvqa/m4c_decoding_bce_with_mask/avg: 1232.3559, train/total_loss: 870.1559, train/total_loss/avg: 1232.3559, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 160, iterations: 160, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 54s 217ms, time_since_start: 27m 59s 950ms, eta: 78h 48m 36s 979ms\n","\u001b[32m2020-12-09T07:52:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 170/25000, train/textvqa/m4c_decoding_bce_with_mask: 870.1559, train/textvqa/m4c_decoding_bce_with_mask/avg: 1168.2311, train/total_loss: 870.1559, train/total_loss/avg: 1168.2311, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 170, iterations: 170, max_updates: 25000, lr: 0.00003, ups: 0.09, time: 01m 52s 480ms, time_since_start: 29m 52s 431ms, eta: 77h 34m 48s 752ms\n","\u001b[32m2020-12-09T07:53:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 180/25000, train/textvqa/m4c_decoding_bce_with_mask: 710.2419, train/textvqa/m4c_decoding_bce_with_mask/avg: 1109.4956, train/total_loss: 710.2419, train/total_loss/avg: 1109.4956, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 180, iterations: 180, max_updates: 25000, lr: 0.00003, ups: 0.15, time: 01m 07s 564ms, time_since_start: 30m 59s 995ms, eta: 46h 34m 54s 309ms\n","\u001b[32m2020-12-09T07:55:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 190/25000, train/textvqa/m4c_decoding_bce_with_mask: 710.2419, train/textvqa/m4c_decoding_bce_with_mask/avg: 1055.6753, train/total_loss: 710.2419, train/total_loss/avg: 1055.6753, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 190, iterations: 190, max_updates: 25000, lr: 0.00004, ups: 0.09, time: 01m 53s 717ms, time_since_start: 32m 53s 713ms, eta: 78h 22m 14s 003ms\n","\u001b[32m2020-12-09T07:57:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.text_bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.text_bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.obj) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.obj) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.ocr) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.ocr) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.mmt) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.mmt) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n","See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n","You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n","  warnings.warn(message=msg, category=UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-12-09T07:57:31 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","\n","\u001b[32m2020-12-09T07:57:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/25000, train/textvqa/m4c_decoding_bce_with_mask: 574.3228, train/textvqa/m4c_decoding_bce_with_mask/avg: 1006.2834, train/total_loss: 574.3228, train/total_loss/avg: 1006.2834, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 25000, lr: 0.00004, ups: 0.08, time: 02m 09s 977ms, time_since_start: 35m 03s 690ms, eta: 89h 32m 24s 181ms\n","\u001b[32m2020-12-09T07:58:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 210/25000, train/textvqa/m4c_decoding_bce_with_mask: 461.2125, train/textvqa/m4c_decoding_bce_with_mask/avg: 960.9015, train/total_loss: 461.2125, train/total_loss/avg: 960.9015, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 210, iterations: 210, max_updates: 25000, lr: 0.00004, ups: 0.15, time: 01m 07s 495ms, time_since_start: 36m 11s 186ms, eta: 46h 28m 41s 932ms\n","\u001b[32m2020-12-09T08:00:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 220/25000, train/textvqa/m4c_decoding_bce_with_mask: 368.2024, train/textvqa/m4c_decoding_bce_with_mask/avg: 919.1407, train/total_loss: 368.2024, train/total_loss/avg: 919.1407, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 220, iterations: 220, max_updates: 25000, lr: 0.00004, ups: 0.09, time: 01m 54s 824ms, time_since_start: 38m 06s 011ms, eta: 79h 02m 15s 390ms\n"],"name":"stdout"},{"output_type":"stream","text":["\u001b[32m2020-12-09T08:02:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 230/25000, train/textvqa/m4c_decoding_bce_with_mask: 291.7499, train/textvqa/m4c_decoding_bce_with_mask/avg: 880.6404, train/total_loss: 291.7499, train/total_loss/avg: 880.6404, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 230, iterations: 230, max_updates: 25000, lr: 0.00004, ups: 0.09, time: 01m 53s 664ms, time_since_start: 39m 59s 676ms, eta: 78h 12m 28s 175ms\n","\u001b[32m2020-12-09T08:03:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 240/25000, train/textvqa/m4c_decoding_bce_with_mask: 230.9453, train/textvqa/m4c_decoding_bce_with_mask/avg: 845.0734, train/total_loss: 230.9453, train/total_loss/avg: 845.0734, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 240, iterations: 240, max_updates: 25000, lr: 0.00004, ups: 0.15, time: 01m 05s 132ms, time_since_start: 41m 04s 808ms, eta: 44h 47m 47s 191ms\n","\u001b[32m2020-12-09T08:05:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/25000, train/textvqa/m4c_decoding_bce_with_mask: 181.4704, train/textvqa/m4c_decoding_bce_with_mask/avg: 812.1628, train/total_loss: 181.4704, train/total_loss/avg: 812.1628, max mem: 7069.0, experiment: run, epoch: 1, num_updates: 250, iterations: 250, max_updates: 25000, lr: 0.00004, ups: 0.09, time: 01m 52s 536ms, time_since_start: 42m 57s 344ms, eta: 77h 22m 07s 982ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cx0tn3xp4X7i"},"source":["this is to resume training"]},{"cell_type":"code","metadata":{"id":"16H5t12u29VJ"},"source":["mmf_run config=projects/m4c/configs/textvqa/defaults.yaml datasets=textvqa model=m4c run_type=train_val checkpoint.resume=True env.save_dir=./save/m4c_final_yay training.num_workers=6 training.batch_size=80 training.max_updates=25000 training.log_interval=10 training.checkpoint_interval=100 training.evaluation_interval=2000\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OZFFHi34e_L"},"source":["this is for validation prediction"]},{"cell_type":"code","metadata":{"id":"DqnGTMsn4dil"},"source":["mmf_predict config=projects/m4c/configs/textvqa/defaults.yaml datasets=textvqa model=m4c run_type=val  training.num_workers=1 checkpoint.resume_file=./save/m4c_final_yay/current.ckpt"],"execution_count":null,"outputs":[]}]}