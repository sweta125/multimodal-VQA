{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/facebookresearch/mmf.git\n",
    "#! cd mmf\n",
    "#! pip install --editable .\n",
    "# ! pip install urllib3==1.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /home/ubuntu/mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option model to pythia\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option config to /home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /home/ubuntu/save/pythia\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/ubuntu/save/pythia/current.ckpt\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 10\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 10\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 100\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 6\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf: \u001b[0mLogging to: /home/ubuntu/save/pythia/train.log\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['dataset=textvqa', 'model=pythia', 'config=/home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml', 'env.save_dir=/home/ubuntu/save/pythia', 'run_type=train_val', 'checkpoint.resume_file=/home/ubuntu/save/pythia/current.ckpt', 'training.log_interval=10', 'training.checkpoint_interval=10', 'training.evaluation_interval=100', 'training.num_workers=6'])\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla K80\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf_cli.run: \u001b[0mUsing seed 26481795\n",
      "\u001b[32m2020-11-13T20:11:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-13T20:11:30 | torchtext.vocab: \u001b[0mLoading vectors from /home/ubuntu/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-11-13T20:11:35 | torchtext.vocab: \u001b[0mLoading vectors from /home/ubuntu/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-11-13T20:11:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-13T20:11:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-13T20:11:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:11:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:11:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:11:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:11:50 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-13T20:11:50 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:12:11 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:12:11 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:12:11 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:12:11 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 400\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 400\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 6\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.trainers.mmf_trainer: \u001b[0mPythia(\n",
      "  (word_embedding): Embedding(75505, 300)\n",
      "  (text_embeddings): ModuleList(\n",
      "    (0): TextEmbedding(\n",
      "      (module): AttentionTextEmbedding(\n",
      "        (recurrent_unit): LSTM(300, 1024, batch_first=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "        (conv2): Conv1d(512, 2, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (image_feature_encoders): ModuleList(\n",
      "    (0): FinetuneFasterRcnnFpnFc7(\n",
      "      (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "  )\n",
      "  (image_feature_embeddings_list): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ImageFeatureEmbedding(\n",
      "        (image_attention_model): AttentionLayer(\n",
      "          (module): TopDownAttention(\n",
      "            (combination_layer): ModalCombineLayer(\n",
      "              (module): NonLinearElementMultiply(\n",
      "                (fa_image): ReLUWithWeightNormFC(\n",
      "                  (layers): Sequential(\n",
      "                    (0): Linear(in_features=2048, out_features=5000, bias=True)\n",
      "                    (1): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (fa_txt): ReLUWithWeightNormFC(\n",
      "                  (layers): Sequential(\n",
      "                    (0): Linear(in_features=2048, out_features=5000, bias=True)\n",
      "                    (1): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (dropout): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (transform): TransformLayer(\n",
      "              (module): LinearTransform(\n",
      "                (lc): Linear(in_features=5000, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ImageFeatureEmbedding(\n",
      "        (image_attention_model): AttentionLayer(\n",
      "          (module): TopDownAttention(\n",
      "            (combination_layer): ModalCombineLayer(\n",
      "              (module): NonLinearElementMultiply(\n",
      "                (fa_image): ReLUWithWeightNormFC(\n",
      "                  (layers): Sequential(\n",
      "                    (0): Linear(in_features=2048, out_features=5000, bias=True)\n",
      "                    (1): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (fa_txt): ReLUWithWeightNormFC(\n",
      "                  (layers): Sequential(\n",
      "                    (0): Linear(in_features=2048, out_features=5000, bias=True)\n",
      "                    (1): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (dropout): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (transform): TransformLayer(\n",
      "              (module): LinearTransform(\n",
      "                (lc): Linear(in_features=5000, out_features=1, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (image_text_multi_modal_combine_layer): ModalCombineLayer(\n",
      "    (module): NonLinearElementMultiply(\n",
      "      (fa_image): ReLUWithWeightNormFC(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=4096, out_features=5000, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (fa_txt): ReLUWithWeightNormFC(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=5000, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): ClassifierLayer(\n",
      "    (module): LogitClassifier(\n",
      "      (f_o_text): ReLUWithWeightNormFC(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=5000, out_features=300, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (f_o_image): ReLUWithWeightNormFC(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=5000, out_features=5000, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (linear_text): Linear(in_features=300, out_features=8001, bias=True)\n",
      "      (linear_image): Linear(in_features=5000, out_features=8001, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): LogitBinaryCrossEntropy()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.utils.general: \u001b[0mTotal Parameters: 173451588. Trained Parameters: 173451588\n",
      "\u001b[32m2020-11-13T20:12:11 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-13T20:12:12 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:12 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:12 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:12 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:12 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:12:13 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T20:14:12 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:14:12 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:14:12 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:14:12 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:14:12 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:14:13 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T20:20:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:20:28 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:20:28 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:21:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 410/24000, train/textvqa/logit_bce: 8.4806, train/textvqa/logit_bce/avg: 8.4806, train/total_loss: 8.4806, train/total_loss/avg: 8.4806, max mem: 5729.0, experiment: run, epoch: 7, num_updates: 410, iterations: 410, max_updates: 24000, lr: 0.00264, ups: 0.02, time: 09m 04s 103ms, time_since_start: 09m 25s 352ms, eta: 356h 32m 21s 114ms\n",
      "\u001b[32m2020-11-13T20:26:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:26:43 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:26:43 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:27:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 420/24000, train/textvqa/logit_bce: 8.0900, train/textvqa/logit_bce/avg: 8.2853, train/total_loss: 8.0900, train/total_loss/avg: 8.2853, max mem: 5729.0, experiment: run, epoch: 7, num_updates: 420, iterations: 420, max_updates: 24000, lr: 0.00268, ups: 0.03, time: 06m 13s 948ms, time_since_start: 15m 39s 300ms, eta: 244h 56m 09s 645ms\n",
      "\u001b[32m2020-11-13T20:30:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:30:04 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:30:04 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:30:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 430/24000, train/textvqa/logit_bce: 8.3227, train/textvqa/logit_bce/avg: 8.2977, train/total_loss: 8.3227, train/total_loss/avg: 8.2977, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 430, iterations: 430, max_updates: 24000, lr: 0.00272, ups: 0.05, time: 03m 20s 701ms, time_since_start: 19m 002ms, eta: 131h 24m 13s 247ms\n",
      "\u001b[32m2020-11-13T20:36:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:36:24 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:36:24 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:37:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 440/24000, train/textvqa/logit_bce: 8.0977, train/textvqa/logit_bce/avg: 8.2477, train/total_loss: 8.0977, train/total_loss/avg: 8.2477, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 440, iterations: 440, max_updates: 24000, lr: 0.00276, ups: 0.03, time: 06m 20s 667ms, time_since_start: 25m 20s 670ms, eta: 249h 07m 33s 353ms\n",
      "\u001b[32m2020-11-13T20:45:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:45:27 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:45:27 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:47:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/24000, train/textvqa/logit_bce: 8.1493, train/textvqa/logit_bce/avg: 8.2280, train/total_loss: 8.1493, train/total_loss/avg: 8.2280, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 450, iterations: 450, max_updates: 24000, lr: 0.0028, ups: 0.02, time: 10m 34s 620ms, time_since_start: 35m 55s 290ms, eta: 415h 08m 50s 922ms\n",
      "\u001b[32m2020-11-13T20:52:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:52:13 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T20:52:13 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T20:54:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 460/24000, train/textvqa/logit_bce: 8.1493, train/textvqa/logit_bce/avg: 8.3215, train/total_loss: 8.1493, train/total_loss/avg: 8.3215, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 460, iterations: 460, max_updates: 24000, lr: 0.00284, ups: 0.02, time: 07m 05s 257ms, time_since_start: 43m 547ms, eta: 278h 04m 15s 270ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-13T21:05:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:05:10 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:05:10 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T21:07:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 470/24000, train/textvqa/logit_bce: 8.1715, train/textvqa/logit_bce/avg: 8.3000, train/total_loss: 8.1715, train/total_loss/avg: 8.3000, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 470, iterations: 470, max_updates: 24000, lr: 0.00288, ups: 0.01, time: 12m 39s 700ms, time_since_start: 55m 40s 248ms, eta: 496h 32m 56s 397ms\n",
      "\u001b[32m2020-11-13T21:17:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:17:34 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:17:34 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T21:20:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 480/24000, train/textvqa/logit_bce: 8.1715, train/textvqa/logit_bce/avg: 8.4213, train/total_loss: 8.1715, train/total_loss/avg: 8.4213, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 480, iterations: 480, max_updates: 24000, lr: 0.00292, ups: 0.01, time: 12m 37s 048ms, time_since_start: 01h 08m 17s 297ms, eta: 494h 36m 18s 722ms\n",
      "\u001b[32m2020-11-13T21:24:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:24:45 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:24:45 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T21:27:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 490/24000, train/textvqa/logit_bce: 8.1715, train/textvqa/logit_bce/avg: 8.3869, train/total_loss: 8.1715, train/total_loss/avg: 8.3869, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 490, iterations: 490, max_updates: 24000, lr: 0.00296, ups: 0.02, time: 07m 16s 765ms, time_since_start: 01h 15m 34s 062ms, eta: 285h 13m 55s 450ms\n",
      "\u001b[32m2020-11-13T21:37:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:37:36 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:37:36 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/24000, train/textvqa/logit_bce: 8.1715, train/textvqa/logit_bce/avg: 8.4024, train/total_loss: 8.1715, train/total_loss/avg: 8.4024, max mem: 5730.0, experiment: run, epoch: 7, num_updates: 500, iterations: 500, max_updates: 24000, lr: 0.003, ups: 0.01, time: 12m 43s 147ms, time_since_start: 01h 28m 17s 210ms, eta: 498h 09m 56s 907ms\n",
      "\u001b[32m2020-11-13T21:40:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:40:16 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 779, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 105, in get\n",
      "    raise Empty\n",
      "queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/bin/mmf_run\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n",
      "  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 122, in run\n",
      "    main(configuration, predict=predict)\n",
      "  File \"/home/ubuntu/mmf/mmf_cli/run.py\", line 56, in main\n",
      "    trainer.train()\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/mmf_trainer.py\", line 123, in train\n",
      "    self.training_loop()\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/core/training_loop.py\", line 31, in training_loop\n",
      "    self.run_training_epoch()\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/core/training_loop.py\", line 135, in run_training_epoch\n",
      "    report, meter = self.evaluation_loop(self.val_loader)\n",
      "  File \"/home/ubuntu/mmf/mmf/trainers/core/evaluation_loop.py\", line 29, in evaluation_loop\n",
      "    for batch in tqdm.tqdm(loader, disable=disable_tqdm):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/std.py\", line 1149, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 363, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 974, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 941, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 787, in _try_get_data\n",
      "    if self._workers_status[worker_id] and not w.is_alive():\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 139, in is_alive\n",
      "    returncode = self._popen.poll()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 22523) is killed by signal: Killed. \n"
     ]
    }
   ],
   "source": [
    "! mmf_run dataset=textvqa model=pythia \\\n",
    "    config=/home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml \\\n",
    "    env.save_dir=/home/ubuntu/save/pythia \\\n",
    "    run_type=train_val \\\n",
    "    checkpoint.resume_file=/home/ubuntu/save/pythia/current.ckpt \\\n",
    "    training.log_interval=10 \\\n",
    "    training.checkpoint_interval=10 \\\n",
    "    training.evaluation_interval=100 \\\n",
    "    training.num_workers=6\n",
    "# checkpoint.resume_file=/home/ubuntu/mmf/data/models/pythia_train_val.pth \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option datasets to textvqa\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option model to pythia\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option config to /home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /home/ubuntu/save/pythia\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /home/ubuntu/save/pythia/current.ckpt\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 6\n",
      "\u001b[32m2020-11-13T21:42:48 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf: \u001b[0mLogging to: /home/ubuntu/save/pythia/train.log\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['dataset=textvqa', 'model=pythia', 'config=/home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml', 'env.save_dir=/home/ubuntu/save/pythia', 'run_type=val', 'checkpoint.resume_file=/home/ubuntu/save/pythia/current.ckpt', 'training.num_workers=6', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla K80\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf_cli.run: \u001b[0mUsing seed 49088137\n",
      "\u001b[32m2020-11-13T21:42:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-13T21:42:53 | torchtext.vocab: \u001b[0mLoading vectors from /home/ubuntu/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-11-13T21:42:58 | torchtext.vocab: \u001b[0mLoading vectors from /home/ubuntu/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-11-13T21:43:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-13T21:43:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-13T21:43:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:34 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:34 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:34 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:43:34 | py.warnings: \u001b[0m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 500\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 500\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 7\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-13T21:43:35 | mmf.common.test_reporter: \u001b[0mPredicting for textvqa\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\u001b[32m2020-11-13T21:43:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "\u001b[32m2020-11-13T21:45:14 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "100%|███████████████████████████████████████████| 40/40 [34:41<00:00, 52.03s/it]\n",
      "\u001b[32m2020-11-13T22:18:16 | mmf.common.test_reporter: \u001b[0mWrote predictions for textvqa to /home/ubuntu/save/pythia/textvqa_pythia_49088137/reports/textvqa_run_val_2020-11-13T22:18:16.json\n",
      "\u001b[32m2020-11-13T22:18:16 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "! mmf_predict dataset=textvqa model=pythia \\\n",
    "    config=/home/ubuntu/mmf/projects/pythia/configs/textvqa/defaults.yaml \\\n",
    "    env.save_dir=/home/ubuntu/save/pythia \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_file=/home/ubuntu/save/pythia/current.ckpt \\\n",
    "    training.num_workers=6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/m4c/configs/stvqa/defaults.yaml\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option datasets to stvqa\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option model to m4c\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to m4c.stvqa.defaults\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./save/m4c_stvqa_predict_val\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option training.num_workers to 1\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf: \u001b[0mLogging to: ./save/m4c_stvqa_predict_val/train.log\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/m4c/configs/stvqa/defaults.yaml', 'datasets=stvqa', 'model=m4c', 'run_type=val', 'checkpoint.resume_zoo=m4c.stvqa.defaults', 'env.save_dir=./save/m4c_stvqa_predict_val', 'training.num_workers=1', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla K80\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf_cli.run: \u001b[0mUsing seed 27384794\n",
      "\u001b[32m2020-11-12T06:32:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100%|█████████| 7.42G/7.42G [05:27<00:00, 22.7MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/annotations/annotations.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/annotations/annotations.tar.gz ]\n",
      "Downloading annotations.tar.gz: 100%|██████| 46.5M/46.5M [00:03<00:00, 15.4MB/s]\n",
      "[ Starting checksum for annotations.tar.gz]\n",
      "[ Checksum successful for annotations.tar.gz]\n",
      "Unpacking annotations.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/defaults/extras.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100%|██████████████| 230k/230k [00:00<00:00, 295kB/s]\n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/stvqa/ocr_en/features/features.tar.gz to /home/ubuntu/.cache/torch/mmf/data/datasets/stvqa/ocr_en/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100%|███████████| 617M/617M [00:26<00:00, 23.4MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "\u001b[32m2020-11-12T06:41:50 | transformers.file_utils: \u001b[0mhttps://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpwnphjch3\n",
      "\u001b[32m2020-11-12T06:41:50 | transformers.file_utils: \u001b[0mcopying /tmp/tmpwnphjch3 to cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-12T06:41:50 | transformers.file_utils: \u001b[0mcreating metadata file for /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-12T06:41:50 | transformers.file_utils: \u001b[0mremoving temp file /tmp/tmpwnphjch3\n",
      "\u001b[32m2020-11-12T06:41:50 | transformers.tokenization_utils: \u001b[0mloading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ubuntu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[32m2020-11-12T06:41:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2020-11-12T06:41:58 | transformers.file_utils: \u001b[0mhttps://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpv4mpwt32\n",
      "\u001b[32m2020-11-12T06:42:06 | transformers.file_utils: \u001b[0mcopying /tmp/tmpv4mpwt32 to cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-12T06:42:06 | transformers.file_utils: \u001b[0mcreating metadata file for /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-12T06:42:06 | transformers.file_utils: \u001b[0mremoving temp file /tmp/tmpv4mpwt32\n",
      "\u001b[32m2020-11-12T06:42:06 | transformers.modeling_utils: \u001b[0mloading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "\u001b[32m2020-11-12T06:42:08 | transformers.modeling_utils: \u001b[0mWeights from pretrained model not used in TextBert: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2020-11-12T06:42:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-12T06:42:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:255: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'\n",
      "  \"No type for scheduler specified even though lr_scheduler is True, \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:13 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/build.py:261: UserWarning: scheduler attributes has no params defined, defaulting to {}.\n",
      "  warnings.warn(\"scheduler attributes has no params defined, defaulting to {}.\")\n",
      "\n",
      "\u001b[32m2020-11-12T06:42:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/m4c/m4c.stvqa.tar.gz to /home/ubuntu/.cache/torch/mmf/data/models/m4c.stvqa.defaults/m4c.stvqa.tar.gz ]\n",
      "Downloading m4c.stvqa.tar.gz: 100%|██████████| 336M/336M [00:14<00:00, 22.6MB/s]\n",
      "[ Starting checksum for m4c.stvqa.tar.gz]\n",
      "[ Checksum successful for m4c.stvqa.tar.gz]\n",
      "Unpacking m4c.stvqa.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mWill load key obj_faster_rcnn_fc7.lc.weight from obj_faster_rcnn_fc7.module.lc.weight\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mWill load key obj_faster_rcnn_fc7.lc.bias from obj_faster_rcnn_fc7.module.lc.bias\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mWill load key ocr_faster_rcnn_fc7.lc.weight from ocr_faster_rcnn_fc7.module.lc.weight\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mWill load key ocr_faster_rcnn_fc7.lc.bias from ocr_faster_rcnn_fc7.module.lc.bias\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-12T06:42:36 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-12T06:42:36 | mmf.common.test_reporter: \u001b[0mPredicting for stvqa\n",
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[32m2020-11-12T06:42:36 | mmf.datasets.processors.processors: \u001b[0mLoading fasttext model now from /home/ubuntu/.cache/torch/mmf/wiki.en.bin\n",
      "\n",
      "\u001b[32m2020-11-12T06:42:48 | mmf.datasets.processors.processors: \u001b[0mFinished loading fasttext model\n",
      "100%|███████████████████████████████████████████| 21/21 [04:36<00:00, 13.15s/it]\n",
      "\u001b[32m2020-11-12T06:47:12 | mmf.common.test_reporter: \u001b[0mWrote predictions for stvqa to /home/ubuntu/11777-Project/Pythia/save/m4c_stvqa_predict_val/stvqa_m4c_27384794/reports/stvqa_run_val_2020-11-12T06:47:12.json\n",
      "\u001b[32m2020-11-12T06:47:12 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "! mmf_predict config=projects/m4c/configs/stvqa/defaults.yaml \\\n",
    "    datasets=stvqa \\\n",
    "    model=m4c \\\n",
    "    run_type=val \\\n",
    "    checkpoint.resume_zoo=m4c.stvqa.defaults \\\n",
    "    env.save_dir=./save/m4c_stvqa_predict_val \\\n",
    "    training.num_workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mmf.sh/docs/projects/m4c/#pretrained-m4c-models%20(important%20for%20arguments!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
